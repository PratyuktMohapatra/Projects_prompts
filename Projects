PROJECT 1

{
  "project_name": "Percona Xtraa DB CLuster Set up",
  "company_context": "Automation scripts were executed on approximately 550 applications for continuous monitoring. These scripts inserted data into a centralized MySQL standalone server.",
  "problem_statement": "The single MySQL server created a single point of failure, risking downtime and performance issues during high data load from automation scripts.",
  "objective": "Implement a high-availability and fault-tolerant database system to ensure continuous data ingestion and reporting availability.",
  "solution_implemented": {
    "database_cluster": "Installed and configured Percona XtraDB Cluster on three Virtual Machines (VMs) to achieve database high availability and data redundancy.",
    "load_balancing": "Configured HAProxy on two VMs to distribute database traffic and ensure failover capability.",
    "virtual_ip_setup": {
      "execution_database_vip": "Assigned one Virtual IP (VIP) for handling automation execution database writes.",
      "reports_database_vip": "Assigned another Virtual IP (VIP) for handling reporting database reads."
    },
    "performance_tuning": "Performed fine-tuning of Percona XtraDB Cluster parameters (such as buffer pool size, write set replication, and flow control) according to workload and performance requirements to optimize stability, throughput, and latency.",
    "monitoring_and_observability": "Set up Percona Monitoring and Management (PMM) for real-time visualization and monitoring of cluster health, query performance, node latency, and replication lag to proactively detect and resolve issues."
  },
  "improvements_over_standalone_mysql": {
    "availability": "Increased from approximately 90% uptime to 99.95% uptime, due to clustering and failover support.",
    "fault_tolerance": "Improved by nearly 100%, as node failures no longer cause downtime or data loss.",
    "read_write_performance": "Improved by around 40%, with parallel query handling and load balancing via HAProxy.",
    "data_consistency": "Achieved 100% real-time synchronization across all nodes using synchronous replication.",
    "maintenance_downtime_reduction": "Reduced planned maintenance downtime by approximately 85%, as operations can continue on other nodes."
  },
  "result": "Achieved high availability for both execution and reporting databases, eliminated single point of failure, and ensured stable data flow from automation scripts across 550 monitored applications.",
  "technologies_used": [
    "MySQL",
    "Percona XtraDB Cluster",
    "HAProxy",
    "Percona Monitoring and Management (PMM)",
    "Linux (VM-based deployment)"
  ],
  "role": "Database & Backend Developer",
  "key_responsibilities": [
    "Configured and deployed Percona XtraDB Cluster across three VMs.",
    "Performed fine-tuning of cluster parameters based on workload and performance requirements.",
    "Set up HAProxy with dual VIP configuration for execution and reporting databases.",
    "Installed and configured Percona Monitoring and Management (PMM) for monitoring cluster performance and availability.",
    "Tested high-availability and failover scenarios to ensure reliability.",
    "Collaborated with automation team to verify continuous data ingestion after migration."
  ]
}

PROJECT 2

{
  "project_name": "K-Batch-Ops",
  "project_description": "An automated, on-demand containerized execution system for running and visually monitoring automation scripts using Kubernetes and Flask.",
  "company_context": "Developers in the automation team required an isolated, scalable environment to execute automation scripts for monitoring web applications and observe the execution in real time.",
  "problem_statement": "Automation scripts were previously executed manually or on shared servers, leading to resource conflicts, limited scalability, and lack of live monitoring capability.",
  "objective": "Design a dynamic, container-based execution system that automatically provisions environments on demand, runs automation scripts, enables live execution viewing, and cleans up resources post-execution.",
  "solution_implemented": {
    "api_trigger": "A REST API endpoint was developed using Flask, deployed with Gunicorn, which initiates the entire workflow when triggered by a developer.",
    "dynamic_pod_creation": "Upon API hit, the Flask application creates a Kubernetes Pod using a custom Docker image that includes a VNC display server and all dependencies required to run the automation JAR.",
    "custom_docker_image": "The image contains pre-installed dependencies, browsers, and the automation JAR execution environment.",
    "live_execution_monitoring": "When the Pod starts, a VNC service is exposed, and Remmina on the developer’s laptop auto-connects to the Pod’s VNC port to allow real-time viewing of the automation execution.",
    "service_creation_and_cleanup": "A Kubernetes Service is created dynamically for Pod access, and after execution completion and data insertion into the database, both the Pod and the Service are automatically deleted.",
    "automation_trigger": "Inside the Pod, the automation JAR is triggered, executing the monitoring scripts for the target web applications."
  },
  "workflow_summary": "Developer → Hits API → Gunicorn triggers Flask app → Flask creates Pod + Service → Pod runs automation JAR → Remmina connects to VNC for live monitoring → On completion, results stored in DB → Pod, Service, and Remmina session terminated.",
  "result": "Enabled on-demand, isolated, and fully automated execution of monitoring scripts with live viewing. Reduced manual intervention by 100%, eliminated shared resource issues, and improved environment setup time by approximately 90%.",
  "technologies_used": [
    "Python Flask",
    "Gunicorn",
    "Docker",
    "Kubernetes",
    "VNC (Remmina)",
    "Java (Automation JAR)",
    "MySQL"
  ],
  "role": "Backend & DevOps Developer",
  "key_responsibilities": [
    "Developed Flask API to automate Pod creation, execution, and cleanup.",
    "Built custom Docker image with VNC server and required automation dependencies.",
    "Integrated Flask with Kubernetes APIs for dynamic Pod and Service management.",
    "Automated live monitoring setup using Remmina for VNC connections.",
    "Optimized execution flow to automatically delete Pods and Services after completion.",
    "Ensured automation results were stored in the database before cleanup."
  ],
  "performance_impact": {
    "environment_setup_time_reduction": "Reduced from ~5 minutes to under 30 seconds (≈90% improvement).",
    "manual_intervention": "Eliminated 100% of manual execution and monitoring setup.",
    "system_scalability": "Enabled parallel execution of multiple isolated automation jobs without conflict.",
    "resource_utilization": "Improved by 70% through dynamic provisioning and cleanup."
  }
}

PROJECT 3

{
  "project_name": "Netraa LH Implementation for National Security Depository Limited (NSDL)",
  "client": "National Security Depository Limited (NSDL)",
  "project_description": "Deployed and configured the Netraa LH monitoring application for NSDL, implementing a full-stack monitoring and automation infrastructure using Docker, Jenkins, and MySQL.",
  "company_context": "The client required continuous monitoring of their critical web applications to ensure high availability, performance, and reliability. Netraa LH, an in-house monitoring solution, was used for this purpose.",
  "problem_statement": "The client lacked an integrated monitoring setup capable of running continuous automation scripts 24/7 and visualizing performance data in real-time.",
  "objective": "Deploy and configure the Netraa LH monitoring ecosystem using containerized microservices, automation scripts, and real-time monitoring infrastructure.",
  "solution_implemented": {
    "application_setup": "Deployed Netraa LH as a set of 13 Docker-based microservices, each container handling a dedicated function of the monitoring system (frontend, backend, scheduler, API, etc.).",
    "database_setup": "Installed and configured a dedicated MySQL server to store monitoring and automation data.",
    "automation_execution_setup": "Configured two additional VMs as automation nodes. Installed Jenkins on one VM and created 35 Jenkins jobs to schedule and manage automation executions.",
    "automation_scripts": "Integrated 16 automation scripts for Node 1 and 16 for Node 2, each representing unique monitoring journeys across the client's web applications.",
    "continuous_execution": "Configured Jenkins pipelines to run automation scripts 24/7, ensuring continuous monitoring of application health and functionality.",
    "data_integration": "Automation results were continuously inserted into the MySQL database and displayed in the Netraa LH monitoring dashboard for visualization and reporting.",
    "graphical_access": "Installed and configured VNC Server and LightDM on both automation VMs to provide graphical access, enabling Chrome browser automation to run and be visually monitored."
  },
  "result": "Successfully implemented a continuous, automated monitoring system for NSDL’s web applications using Netraa LH. Achieved 24/7 real-time monitoring with automated data flow from execution to visualization, reducing manual monitoring effort by 100%.",
  "technologies_used": [
    "Docker (13 microservices)",
    "Jenkins (CI/CD Automation)",
    "MySQL",
    "VNC Server",
    "LightDM",
    "Google Chrome (for browser automation)",
    "Linux"
  ],
  "role": "DevOps & Automation Engineer",
  "key_responsibilities": [
    "Deployed Netraa LH microservices architecture using 13 Docker containers.",
    "Installed and configured MySQL database for data storage and integration with Netraa LH.",
    "Set up Jenkins and created 35 jobs to automate execution of 32 monitoring scripts across two nodes.",
    "Configured 24/7 automation pipeline for continuous monitoring.",
    "Installed and managed VNC Server and LightDM for graphical access to automation environments.",
    "Ensured all automation results were successfully pushed to the database and reflected in the Netraa LH dashboard."
  ],
  "performance_impact": {
    "automation_coverage": "Achieved 100% coverage of NSDL's monitored web applications.",
    "manual_effort_reduction": "Reduced manual monitoring effort by 100% through automated, continuous execution.",
    "monitoring_reliability": "Increased monitoring reliability by 95% through automated error recovery and consistent data updates.",
    "deployment_efficiency": "Reduced deployment time by 80% using Docker-based microservices architecture."
  }
}


{
  "technologies": [
    "Linux",
    "Docker",
    "Jenkins",
    "Kubernetes",
    "Ansible",
    "AWS",
    "Azure",
    "Proxmox",
    "Sophos",
    "CloudStack",
    "Java",
    "Python",
    "Git",
    "GitLab",
    "GitHub",
    "VirtualBox",
    "VMware",
    "Bash",
    "Percona XtraDB Cluster",
    "x11vnc",
    "LightDM",
    "MySQL Server",
    "Trivy",
    "Harbor"
  ]
}

